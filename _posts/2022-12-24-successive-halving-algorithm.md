---
title: Successive Halving Algorithm
tags: [hpo, sha, multi-fidelity]
category: ML
layout: post
---

ğŸ“„ Jamieson, Kevin, and Ameet Talwalkar. "Non-stochastic best arm identification and hyperparameter optimization." *Artificial intelligence and statistics*. PMLR, 2016.

<!--more-->

## Non-stochastic Best Arm Identification

Successive Halving Algorithmì€ Bandit ê¸°ë°˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê¸°ë²•ì…ë‹ˆë‹¤. Banditì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ë“¤ìœ¼ë©´ ê°€ì¥ ë¨¼ì € ë– ì˜¤ë¥´ëŠ” ê²ƒì´ A/B í…ŒìŠ¤íŠ¸ì— ë§ì´ ì‚¬ìš©ë˜ëŠ” Multi-armed Banditì…ë‹ˆë‹¤. ì—¬ëŸ¬ ê°œì˜ ìŠ¬ë¡¯ë¨¸ì‹ ì´ ìˆê³ , ê·¸ ìŠ¬ë¡¯ë¨¸ì‹ ì˜ íŒ” (arm)ì„ ë‹¹ê²¨ì„œ ë³´ìƒ (reward)ì„ ì–»ê²Œ ë©ë‹ˆë‹¤. ì´ ë³´ìƒì€ ì–´ë–¤ í™•ë¥  ë¶„í¬ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ë°, ì–´ë–¤ ì „ëµìœ¼ë¡œ ìŠ¬ë¡¯ë¨¸ì‹ ì„ ê³¨ë¼ì•¼ ìµœëŒ€ì˜ ë³´ìƒì„ ì–»ëŠ”ì§€ì— ê´€í•œ ë‚´ìš©ì´ ë°”ë¡œ MABì…ë‹ˆë‹¤. MAB ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ ê°œë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ”ë°ìš”. í•˜ë‚˜ëŠ” ê°€ì¥ í° ë³´ìƒì„ ì£¼ëŠ” ìµœì„ ì˜ ìŠ¬ë¡¯ë¨¸ì‹ ì˜ íŒ”ì„ ì°¾ëŠ” ê²ƒì´ê³ , í•˜ë‚˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ë‹¤ë£¨ëŠ” Exploration-Exploitation Trade-offë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë³¸ ê¸€ì—ì„œ ë‹¤ë£¨ëŠ” Successive Halving Algorithmì€ ì²« ë²ˆì§¸ ë°©ë²•ì¸ **Best Arm Identification**ì„ ê¸°ë°˜ì— ë‘ê³  ìˆìŠµë‹ˆë‹¤.

### Stochastic vs Non-stochastic

Best Arm Identification ë°©ë²•ë„ í¬ê²Œ Stochasticí•œ í™˜ê²½ê³¼ Non-stochasticí•œ í™˜ê²½ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ë ‡ê²Œ í‘œí˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤.

>ğŸ° **Stochastic**
>
>ëª¨ë“  $i \in [n], k \geq 1$ì— ëŒ€í•´ì„œ $\mathbb{E}[\ell_{i, k}] = \mu_i$ë¥¼ ë§Œì¡±í•˜ëŠ” $\ell_{i,k}$ë¥¼ $[0, 1]$ êµ¬ê°„ ë‚´ì˜ **í™•ë¥  ë¶„í¬ì—ì„œ ë½‘ì€ i.i.d ìƒ˜í”Œ**ë¡œ ë‘ì.
>ì´ë•Œ $\sum^n_{i=1} T_i$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë™ì•ˆ $\arg\min_i \mu_i$ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.
>
>ğŸ¯ **Non-stochastic**
>
>ëª¨ë“  $i \in [n], k\geq 1$ì— ëŒ€í•´ì„œ **ì•Œê³ ë¦¬ì¦˜ì˜ í–‰ë™ì— ë…ë¦½ì ì¸ Loss sequence $\ell_{i, k} \in \mathbb{R}$ì„ ìƒì„±**í•œë‹¤ê³  í•˜ì. ë” ë‚˜ì•„ê°€ $\nu_i = \lim_{\tau \to \infty} \ell_{i, \tau}$ê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•˜ì.
>ì´ë•Œ $\sum^n_{i=1} T_i$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë™ì•ˆ $\arg\min_{i} \nu_i$ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.

í‘œí˜„ì´ ì–´ë µì§€ë§Œ ê° í™˜ê²½ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì€ **$\ell_{i, k}$ë¥¼ ì–´ë–»ê²Œ ì •ì˜í•˜ëŠëƒ**ì…ë‹ˆë‹¤. Stochasticí•œ ë°©ë²•ì€ $\ell_{i, k}$ë¥¼ í™•ë¥  ë¶„í¬ì—ì„œ ë½‘ì€ i.i.d ìƒ˜í”Œì„, Non-stochasticí•œ ë°©ë²•ì—ì„œëŠ” ì•Œê³ ë¦¬ì¦˜ì— ë…ë¦½ì ì¸ ì–´ë–¤ ìˆœì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ëŠ” Non-stochastic í™˜ê²½ì—ì„œì˜ Bandit ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¬ë¡¯ë¨¸ì‹ ì˜ íŒ”ì€ í•˜ë‚˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ìœ¼ë¡œ, $\ell_{i, k}$ëŠ” í•™ìŠµí•˜ëŠ” ëª¨ë¸ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ non-stochasticí•˜ê³  obliviousí•œ ê²°ê³¼ë¥¼ ì–»ê²Œ ë˜ëŠ”ë°ìš”. ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì€ **í™•ë¥ ì ì´ ì•„ë‹Œ ê²°ì •ë¡ ì (deterministic)ìœ¼ë¡œ** ë‚˜ì˜¤ê³ , ê°ê°ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ê³¼ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì— ëŒ€í•œ **ì •ë³´ë¥¼ ê³µìœ í•˜ì§€ ì•Šê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ì´ëŸ° ê´€ì ì—ì„œ ë³¸ ë…¼ë¬¸ì—ì„œë„ Non-stochasticí•œ Best Arm Identificationìœ¼ë¡œ í”„ë ˆì´ë°í•˜ì—¬ ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤.

### Non-stochasticì´ ì‰½ì§€ ì•Šì€ ì´ìœ 

Stochasticí•œ í™˜ê²½ì€ ëª‡ ê°€ì§€ ì„¤ì •ì„ í†µí•´ Non-stochasticí•œ í™˜ê²½ìœ¼ë¡œ ë°”ë€” ìˆ˜ ìˆìŠµë‹ˆë‹¤. Stochasticí•œ í™˜ê²½ì—ì„œì˜ Loss ê°’ì¸ $\ell^\prime\_{i, T_i}$ ë¥¼ ì´ìš©í•´ $\ell\_{i, T_i} = \frac{1}{T_i} \sum^{T_i}_{k=1} \ell^\prime\_{i, T_i}$ ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤. ì´ ë•Œ í° ìˆ˜ì˜ ë²•ì¹™ì— ì˜í•´ ë‹¤ìŒì´ ì„±ë¦½í•©ë‹ˆë‹¤.

$$
\lim_{\tau \to \infty} \ell_{i, \tau} = \mathbb{E}[\ell^\prime_{i, 1}].
$$

ê·¸ë¦¬ê³  $\ell\_{i, T_i} = \min \left\\{ \ell^\prime\_{i, 1}, \cdots, \ell^\prime\_{i, T_i} \right \\}$ ë¡œ ë‘”ë‹¤ë©´ $\ell\_{i, t}$ ëŠ” bounded, monotonically decreasing sequenceê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ $\ell\_{i, t}$ ëŠ” [Monotone Convergence Theorem](https://en.wikipedia.org/wiki/Monotone_convergence_theorem)ì— ì˜í•´ ë°˜ë“œì‹œ ê·¹í•œê°’ì„ ê°–ê²Œ ë©ë‹ˆë‹¤.

Stochasticí•œ í™˜ê²½ê³¼ Non-stochasticí•œ í™˜ê²½ì˜ ì°¨ì´ëŠ” ê·¹í•œê°’ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ì†ë„ì—ì„œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. Stochasticí•œ í™˜ê²½ì€ ìˆ˜ë ´í•˜ëŠ” ì†ë„ë¥¼ ì•Œ ìˆ˜ ìˆì§€ë§Œ Non-stochastic í™˜ê²½ì€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Stochasticí•œ ì„¤ì •ì—ì„œ $\hat{\mu}\_{i, T_i} = \frac{1}{T_i} \sum^{T_i}\_{k=1} \ell\_{i, k}$ë¡œ ë’€ì„ ë•Œ ëª¨ë“  $i \in [n], T_i > 0$ ì— ëŒ€í•´ ë‹¤ìŒì„ ë§Œì¡±í•©ë‹ˆë‹¤.

$$
\left| \hat{\mu}_{i, T_i} - \mu_i \right| \leq \sqrt{\frac{\log(4nT_i^2)}{2T_i}}.
$$


>ğŸ§ **ì´ ì‹ì´ ì„±ë¦½í•˜ëŠ” ì´ìœ ëŠ”?**
>
>*$X_t$ê°€ $[0, 1]$ ì•ˆì— ìˆê³ , $n > 1$ ì¼ ë•Œ ë‹¤ìŒì´ ì„±ë¦½í•œë‹¤.*
>
>$$ P \left( \bigcup^\infty_{t=1} \left\{ \left| \frac{1}{t} \sum^t_{s=1} X_s - \mathbb{E}[X_s] \right| \geq \sqrt{\frac{\log(4nt^2)}{2t}} \right\} \right) \leq \frac{1}{n}. $$
>
>**ì¦ëª….**
>
>[Boole's Inequality](https://en.wikipedia.org/wiki/Boole%27s_inequality)ì— ì˜í•´ì„œ ë‹¤ìŒì´ ì„±ë¦½í•©ë‹ˆë‹¤.
>
>$$ P \left( \bigcup^\infty_{t=1} \left\{ \left| \frac{1}{t} \sum^t_{s=1} X_s - \mathbb{E}[X_s] \right| \geq \sqrt{\frac{\log(4nt^2)}{2t}} \right\} \right) \leq \sum^\infty_{t=1} P \left( \left| \frac{1}{t} \sum^t_{s=1} X_s - \mathbb{E}[X_s] \right| \geq \sqrt{\frac{\log(4nt^2)}{2t}} \right). $$
>
>[Hoeffding's Inequality](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality)ì™€ ë¬´í•œ ê¸‰ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>
>$$\begin{aligned}
>\sum^\infty_{t=1} P \left( \left| \frac{1}{t} \sum^t_{s=1} X_s - \mathbb{E}[X_s] \right| \geq  \sqrt{\frac{\log(4nt^2)}{2t}} \right) &\leq \sum^\infty_{t=1} 2e^{-\log(4nt^2)} \\
>&\leq \sum^\infty_{t=1} \frac{2}{4nt^2} \\ 
>&\leq \sum^\infty_{t=1} \frac{1}{2nt^2} \\
>&\leq \frac{1}{2n} \sum^\infty_{t=1} \frac{1}{t^2} \\
>&= \frac{1}{2n} \cdot \frac{\pi^2}{6} \\
>&\leq \frac{1}{2n} \cdot 2 = \frac{1}{n} &\blacksquare
>\end{aligned}$$

Non-stochasticí•œ í™˜ê²½ì—ì„œì˜ ê°€ì •ì€ $\lim_{\tau \to \infty} \ell_{i, \tau}$ ê°€ ì¡´ì¬í•˜ë©´ ë‹¤ìŒì„ ë§Œì¡±í•˜ëŠ” ì¦ê°€í•˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ $\gamma_i$ ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

$$
\left| \ell_{i, t} - \lim_{\tau \to \infty} \ell_{i, \tau} \right| \leq \gamma_{i}(t) \to 0 \quad \text{ as} \quad t \to \infty
$$

ìœ„ ë‚´ìš©ì€ ê·¹í•œê°’ìœ¼ë¡œì˜ ìˆ˜ë ´ì€ ë³´ì¥í•´ì£¼ì§€ë§Œ **ì–¼ë§ˆë‚˜ ë¹¨ë¦¬** $\gamma_i(t)$ ê°€ 0ìœ¼ë¡œ ë„ë‹¬í•˜ëŠ”ì§€ëŠ” ì•Œë ¤ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ì‚¬ì‹¤ì€ ë‹¤ìŒì˜ ë‘ ê°€ì§€ ê²°ê³¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1.   ê²°êµ­ Best Armì´ ì¡´ì¬í•˜ê¸°ëŠ” í•œë‹¤.
2.   Best Armì¸ì§€ í™•ì¸í•  ìˆ˜ ì—†ê±°ë‚˜ Best Armì˜ ì •í™•í•œ ê°’ì„ ì•Œì•„ë‚¼ ìˆ˜ ì—†ë‹¤.

ë³¸ ë…¼ë¬¸ì€ ê¸°ì¡´ì— ì œì•ˆë˜ì—ˆë˜ Successive Halving Algorithmì— ëŒ€í•´ ìœ„ ë‚´ìš©ì„ í†µí•´ ì •í•´ì§„ ì˜ˆì‚°(budget) ë‚´ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ëŠ” íš¨ê³¼ì ì´ê³  íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

## Successive Halving Algorithm

![Pseudo-code for SHA](/assets/images/2022-12-24-successive-halving-algorithm/sha_algorithm.png){: w="600"}
_Pseudo-code for SHA_

Successive Halving Algorithmì€ ë‹¤ìŒ ìˆœì„œë¡œ ì‘ë™í•©ë‹ˆë‹¤.

-   ì…ë ¥ê°’ìœ¼ë¡œ ì˜ˆì‚° $B$ ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ê°’ ê°œìˆ˜ $n$ ì„ ê°–ìŠµë‹ˆë‹¤. 
    -   ë‘ ê°œì˜ ì…ë ¥ê°’ ì¤‘ ì˜ˆì‚° $B$ ëŠ” $B \leftarrow n$, $B \leftarrow 2B$ ë¡œ ì„¤ì •í•˜ëŠ” ë”ë¸”ë§ íŠ¸ë¦­ (doubling trick)ìœ¼ë¡œ ì—†ì•¨ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
-   ê·¸ ë‹¤ìŒ ìµœì´ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì§‘í•© $S_0$ ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
-   ë‹¤ìŒì˜ ê³¼ì •ì„ í•˜ë‚˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì´ ë‚¨ì„ ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
    -   $S_k$ ì— ë‚¨ì•„ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ë“¤ë¡œ ëª¨ë¸ì„ $r_k$ Epoch ë§Œí¼ ë” í•™ìŠµí•˜ê³  $R_k$ ë¥¼ ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ Epoch ìˆ˜ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    -   í•™ìŠµí•œ ëª¨ë¸ ì¤‘ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ ê°’ì¸ $\ell_{i, k}$ ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì€ ì ˆë°˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ê°’ì„ ë²„ë¦½ë‹ˆë‹¤.
    -   ë‚¨ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ $S_{k+1}$ ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
-   ë§ˆì§€ë§‰ìœ¼ë¡œ ë‚¨ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

### ì‹¤ì œ ì˜ˆì‹œ

![Figure from [1]](/assets/images/2022-12-24-successive-halving-algorithm/sha_example.png){: w="500"}
_Figure from [1]_

ì´í•´ë¥¼ ìœ„í•´ì„œ ì§ì ‘ ê°’ì„ ëŒ€ì…í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ìš°ì„  ì˜ˆì‚° $B$ ë¥¼ 32ë¡œ, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ê°’ ê°œìˆ˜ $n$ ì€ 8ë¡œ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ìµœì´ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì§‘í•© $S_0$ ì˜ í¬ê¸°ëŠ” ë‹¹ì—°íˆ 8ì´ê³ , ì•Œê³ ë¦¬ì¦˜ì˜ ë°˜ë³µì€ $k=0$ ë¶€í„° $k = \lfloor \log_2(n) \rfloor -1 = 2$ ê¹Œì§€ ì§„í–‰í•©ë‹ˆë‹¤.

1.   $k=0$
     -   $r_0 = \lfloor \frac{32}{ \|S_0\| \lceil \log_2(8) \rceil} \rfloor = \lfloor \frac{32}{8 \cdot 3} \rfloor = 1$
     -   $S_0$ ë‚´ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ 1 Epoch ë§Œí¼ í•™ìŠµí•˜ê³ , ì„±ëŠ¥ì´ ë‚®ì€ ì ˆë°˜ì„ ë²„ë¦½ë‹ˆë‹¤.
     -   ë”°ë¼ì„œ $\|S_1\| = 4$ ê°€ ë©ë‹ˆë‹¤.
2.   $k=1$
     -   $r_1 = \lfloor \frac{32}{\|S_1\| \lceil \log_2(8) \rceil} \rfloor = \lfloor \frac{32}{4 \cdot 3} \rfloor = 2$
     -   $S_1$ ë‚´ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ 2 Epochs ë§Œí¼ í•™ìŠµí•˜ê³ , ì„±ëŠ¥ì´ ë‚®ì€ ì ˆë°˜ì„ ë²„ë¦½ë‹ˆë‹¤.
     -   ë”°ë¼ì„œ $\|S_2\| = 2$ ê°€ ë©ë‹ˆë‹¤.
3.   $k=2$
     -   $r_2 = \lfloor \frac{32}{\|S_2\| \lceil \log_2(8) \rceil} \rfloor = \lfloor \frac{32}{2 \cdot 3} \rfloor = 5$
     -   $S_2$ ë‚´ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ 5 Epochs ë§Œí¼ í•™ìŠµí•˜ê³ , ì„±ëŠ¥ì´ ë‚®ì€ ì ˆë°˜ì„ ë²„ë¦½ë‹ˆë‹¤.
     -   ë”°ë¼ì„œ $\|S_3\| = 1$ì´ ë˜ê³ , $S_3$ì— ì¡´ì¬í•˜ëŠ” ë‹¨ í•˜ë‚˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ SHAì—ì„œ ì°¾ëŠ” ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

### SHAì˜ ìˆ˜ë ´ì„± ë³´ì¥

>   âš ï¸ ì•„ë˜ ë‚´ìš©ì€ ìì„¸í•œ ì´í•´ë¥¼ ì›í•˜ì‹œëŠ” ê²½ìš°ì—ë§Œ ë³´ì‹œë©´ ë©ë‹ˆë‹¤. í•„ìˆ˜ì ì¸ ë‚´ìš©ì€ ì•„ë‹™ë‹ˆë‹¤. ğŸ˜€

ë³´ê¸°ì—ëŠ” ê°„ë‹¨í•´ë³´ì´ëŠ”ë° ì´ ì•Œê³ ë¦¬ì¦˜ì´ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì˜ ì°¾ì•„ì¤€ë‹¤ëŠ” ë³´ì¥, ì¦‰ **ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ì„± ë³´ì¥**ì€ ì–´ë–»ê²Œ ë˜ëŠ”ê±¸ê¹Œìš”? ë…¼ë¬¸ì—ì„œëŠ” ì—¬ëŸ¬ ì •ë¦¬(Theorem)ë¥¼ ì´ìš©í•˜ì—¬ ìˆ˜ë ´ì„±ì— ëŒ€í•œ ë‚´ìš©ì„ ì—„ë°€í•˜ê²Œ ë‹¤ë£¨ê³  ìˆëŠ”ë°, ë³¸ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì•½ì‹ìœ¼ë¡œ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

#### (1) ì „ì²´ ìƒ˜í”Œì˜ ìˆ˜ê°€ ì˜ˆì‚°ì„ ë„˜ì§€ ì•ŠìŒ

ìš°ì„  ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë°œìƒí•˜ëŠ” ì „ì²´ ìƒ˜í”Œì´ ì˜ˆì‚° $B$ë¥¼ ë„˜ì§€ ì•ŠëŠ” ê²ƒë¶€í„° ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ ë„˜ê²Œ ëœë‹¤ë©´ ìˆ˜ë ´ì€ ë‘˜ì§¸ì¹˜ê³  ì•Œê³ ë¦¬ì¦˜ì˜ ìœ íš¨ì„±ì´ ì‚¬ë¼ì§€ë‹ˆê¹Œìš”. ì „ì²´ ìƒ˜í”Œì˜ ìˆ˜ëŠ” ë§¤ ë‹¨ê³„ë§ˆë‹¤ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì§‘í•© ë‚´ ì›ì†Œì˜ ìˆ˜ì™€ í•™ìŠµí•  Epoch ìˆ˜ë¥¼ ê³±í•œ ê²ƒê³¼ ê°™ìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$$
\text{\#Samples} = \sum_{k=0}^{\lceil \log_2(n) \rceil -1} |S_k| \left\lfloor \frac{B}{|S_k| \lceil \log_2(n) \rceil} \right\rfloor
$$

ì—¬ê¸°ì„œ floor functionì˜ ì„±ì§ˆì„ ì´ìš©í•´ì•¼ í•©ë‹ˆë‹¤.. ë¶„ëª¨ì— ìˆëŠ” $\|S_k\|$ ëŠ” ìì—°ìˆ˜ì´ê¸° ë•Œë¬¸ì— ì´ ê°’ì„ floor function ë°–ìœ¼ë¡œ ëºì„ ë•Œ ì†Œìˆ«ì  ì•„ë˜ ìˆ˜ê°€ ë°œìƒí•´ì„œ $\|S_k\|$ ë¥¼ floor function ë°–ìœ¼ë¡œ ëºì„ ë•Œê°€ ë” í° ìˆ˜ê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{aligned}
\sum_{k=0}^{\lceil \log_2(n) \rceil -1} |S_k| \left\lfloor \frac{B}{|S_k| \lceil \log_2(n) \rceil} \right\rfloor &\leq \sum_{k=0}^{\lceil \log_2(n) \rceil -1} |S_k| \frac{1}{|S_k|} \left\lfloor \frac{B}{\lceil \log_2(n) \rceil} \right\rfloor \\
&= \sum_{k=0}^{\lceil \log_2(n) \rceil -1} \left\lfloor \frac{B}{\lceil \log_2(n) \rceil} \right\rfloor \\
& \leq \sum_{k=0}^{\lceil \log_2(n) \rceil -1} \frac{B}{\lceil \log_2(n) \rceil} = B
\end{aligned}
$$

ë”°ë¼ì„œ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë°œìƒí•˜ëŠ” ì „ì²´ ìƒ˜í”Œì˜ ìˆ˜ëŠ” ì˜ˆì‚°ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìŠµë‹ˆë‹¤.

#### (2) ìˆ˜ë ´ì„± ë³´ì¥

ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • $i = 1, 2, \cdots, n$ì— ëŒ€í•´ì„œ ë‹¤ìŒì„ ì •ì˜í•©ë‹ˆë‹¤.

$$
\nu_i = \lim_{\tau \to \infty} \ell_{i, \tau}
$$

ì´ ê°’ì€ ìœ„ì—ì„œì˜ Non-stochasticí•œ í™˜ê²½ì—ì„œì˜ ê°€ì •ì— ì˜í•´ ë°˜ë“œì‹œ ì¡´ì¬í•˜ëŠ” ê°’ì…ë‹ˆë‹¤. ì¼ë°˜ì„±ì„ ìƒì§€ ì•Šê³  ë‹¤ìŒê³¼ ê°™ì´ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ê° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì¬ì •ë ¬í–ˆë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.

$$
\nu_1 \leq \nu_2 \leq \cdots \leq \nu_n.
$$

ê·¸ë¦¬ê³  ê°ê°ì˜ $i = 1, 2, \cdots, n$ì— ëŒ€í•´ì„œ ë‹¤ìŒì„ ë§Œì¡±í•˜ëŠ” point-wise smallest, non-increasingí•œ $t$ì— ëŒ€í•œ í•¨ìˆ˜ $\gamma_i(t)$ ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

$$
\left| \ell_{i, t} - \nu_i \right| \leq \gamma_i(t) \quad \forall t. \tag{*}
$$

$\gamma_i(t)$ ëŠ” $\ell_{i, t}$ ì™€ ê·¸ ê·¹í•œê°’ $\nu_i$ ì˜ ì°¨ì´ë¥¼ boundí•˜ëŠ” ì–´ë–¤ í•¨ìˆ˜ê°€ ë©ë‹ˆë‹¤. ì—¬ê¸°ê¹Œì§€ ì •ì˜ê°€ ë˜ì—ˆë‹¤ë©´ ì„ì˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • $i$ ì˜ ì–´ë–¤ ì‹œì  $t_i$ ì—ì„œì˜ ì†ì‹¤ í•¨ìˆ˜ê°’ $\ell\_{i, t_i}$ ì™€ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ê²Œ ë‚˜ì˜¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì˜ ì–´ë–¤ ì‹œì  $t_1$ ì—ì„œì˜ ì†ì‹¤ í•¨ìˆ˜ê°’ $\ell\_{1, t_1}$ ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•´ë´…ì‹œë‹¤. 

$$
\begin{aligned}
\ell_{i, t_i} - \ell_{1, t_1} &= (\ell_{i, t_i} - \nu_i) + (\nu_1 - \ell_{1, t_1}) + 2\left(\frac{\nu_i - \nu_1}{2}\right) \\
& \geq -\gamma_i(t_i) - \gamma_1(t_1) + 2\left(\frac{\nu_i - \nu_1}{2}\right) & \text{by (*)}
\end{aligned}
$$

ì—¬ê¸°ì„œ $\gamma_i(t)$ ì— ëŒ€í•œ ìœ ì‚¬ ì—­í•¨ìˆ˜ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.

$$
\gamma_i^{-1}(\alpha) = \min \{ t \in \mathbb{N} \mid \gamma_i(t) \leq \alpha \} \quad \forall i \in [n]
$$

ë§Œì•½ $t_i > \gamma_i^{-1} \left( \frac{\nu_i - \nu_1}{2} \right)$ì´ë©´ì„œ $t_1 > \gamma_1^{-1} \left( \frac{\nu_i - \nu_1}{2} \right)$ ë¼ë©´ $\gamma_i(t)$ ëŠ” non-increasing functionì´ê¸° ë•Œë¬¸ì— ë‹¤ìŒì´ ì„±ë¦½í•©ë‹ˆë‹¤.

$$
\gamma_i(t_i) < \frac{\nu_i - \nu_1}{2} \quad \text{and} \quad \gamma_1(t_1) < \frac{\nu_i - \nu_1}{2}
$$

ë”°ë¼ì„œ ì‹ (12)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{aligned}
\ell_{i, t_i} - \ell_{1, t_1} &= (\ell_{i, t_i} - \nu_i) + (\nu_1 - \ell_{1, t_1}) + 2\left(\frac{\nu_i - \nu_1}{2}\right) \\
& \geq -\gamma_i(t_i) - \gamma_1(t_1) + 2\left(\frac{\nu_i - \nu_1}{2}\right) > 0
\end{aligned}
$$

ë³´ë‹¤ ì—„ë°€í•˜ê²Œ ë§í•˜ìë©´ ë‹¤ìŒì´ ì„±ë¦½í•©ë‹ˆë‹¤.

$$
\min \{ t_i, t_1 \} > \max \left\{ \gamma_i^{-1} \left( \frac{\nu_i - \nu_1}{2} \right), \gamma_1^{-1} \left( \frac{\nu_i - \nu_1}{2} \right) \right\} \implies \ell_{i, t_i} > \ell_{1, t_i}
$$

ì´ë¥¼ í†µí•´ ì ë‹¹í•œ ì„ì˜ì˜ ë‘ ì‹œì , $t_i$ì™€ $t_1$ì—ì„œ $\ell_{i, t_i} > \ell_{1, t_1}$ ê°€ ì„±ë¦½í•œë‹¤ë©´ í•™ìŠµ ì¤‘ê°„ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ìµœì¢… ìˆ˜ë ´ê°’ì¸ $\nu_i$ì™€ $\nu_1$ì˜ ëŒ€ì†Œ ê´€ê³„ë¥¼ ë¹„êµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³¸ë‹¤ë©´ $\gamma_i(t)$ê°€ ë‘ ìˆ˜ë ´ê°’ $\nu_i$ì™€ $\nu_1$ì˜ í‰ê· ë³´ë‹¤ ì‘ì•„ì§€ê²Œ ë˜ëŠ” $t$ëŠ” **ì¶©ë¶„íˆ í° ìˆ˜**ê°€ ë  ê²ƒì…ë‹ˆë‹¤. $t$ë¥¼ í¬ê²Œ ì¡ê¸° ìœ„í•´ì„  ì´ ì˜ˆì‚° $B$ë¥¼ ì¶©ë¶„íˆ í¬ê²Œ ì¡ì•„ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì¶©ë¶„íˆ í° $B$ ê°’ì„ ê°–ëŠ”ë‹¤ë©´ ì´ëŠ” ê²°êµ­ SHAê°€ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ì°¾ì•„ë‚´ê¸°ì— ì¶©ë¶„í•˜ê²Œ ë©ë‹ˆë‹¤.

## ë ˆí¼ëŸ°ìŠ¤

[1] Feurer, Matthias, and Frank Hutter. "Hyperparameter optimization." *Automated machine learning*. Springer, Cham, 2019. 3-33.